# ü§ñ Machine Learning ‚Äì Detailed Notes for Interviews & Practice

This document provides a **structured and detailed overview of Machine Learning concepts**, mathematical foundations, algorithms, and practical implementations. It is organized in a **theory + formula + intuition + use case** manner, ideal for interviews and project prep.

---

## 1. Introduction to Machine Learning and Key Concepts

- **Artificial Intelligence (AI)**  
  - Broadest concept ‚Üí Systems performing tasks without human intervention.  
  - Examples: Netflix recommendations, Amazon product suggestions, YouTube ads, Tesla self-driving.  

- **Machine Learning (ML)**  
  - Subset of AI.  
  - Uses statistical techniques to **analyze, learn, and predict from data**.  

- **Deep Learning (DL)**  
  - Subset of ML.  
  - Uses **multi-layered neural networks** to mimic brain-like structures.  
  - Effective for images, speech, and NLP.  

- **Data Science (DS)**  
  - Encompasses **data analysis + ML + DL**.  
  - Goal = Solve **business problems** end-to-end.  

### Types of ML
1. **Supervised Learning**  
   - Data: Input + Output labels.  
   - **Regression** ‚Üí Predict continuous values (e.g., predict weight from age).  
   - **Classification** ‚Üí Predict discrete categories (e.g., pass/fail).  

2. **Unsupervised Learning**  
   - Data: Only Input (no labels).  
   - **Clustering** ‚Üí Group similar points (e.g., customer segmentation).  
   - **Dimensionality Reduction** ‚Üí Reduce feature space (e.g., PCA).  

3. **Reinforcement Learning (RL)**  
   - Agent learns via **rewards/penalties**.  
   - Example: Robotics, games.  

### Algorithms Covered
- **Regression**: Linear, Ridge, Lasso  
- **Classification**: Logistic Regression, Naive Bayes, Decision Tree, Random Forest, SVM  
- **Ensemble**: AdaBoost, Gradient Boosting, XGBoost  
- **Instance-based**: KNN  
- **Unsupervised**: K-Means, Hierarchical Clustering, DBSCAN  
- **Dimensionality Reduction**: PCA, LDA  

---

## 2. Linear Regression: Theory, Mathematics & Performance Metrics
‚è±Ô∏è *[18:09 ‚Üí 56:05]*

- **Linear Regression Model**:  
  \[
  y = \theta_0 + \theta_1 x
  \]

- **Cost Function (MSE / Least Squares)**:  
  \[
  J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \big(h_\theta(x_i) - y_i\big)^2
  \]

- **Gradient Descent Update Rule**:  
  \[
  \theta_j := \theta_j - \alpha \frac{\partial J}{\partial \theta_j}
  \]

  - Œ± = Learning rate  
  - Convex cost function ‚Üí No local minima issues.  

### Performance Metrics
- **R¬≤ (Coefficient of Determination)**: Proportion of variance explained.  
- **Adjusted R¬≤**: Penalizes irrelevant features ‚Üí avoids artificial inflation.  

üìå **Regression vs Classification**  
- Regression ‚Üí Continuous output.  
- Classification ‚Üí Categorical output.  

---

## 3. Ridge & Lasso Regression (Regularization)
‚è±Ô∏è *[56:05 ‚Üí 01:27:44]*

- **Ridge Regression (L2 penalty)**:  
  \[
  J(\theta) = \text{MSE} + \lambda \sum_j \theta_j^2
  \]  
  Shrinks coefficients, reduces overfitting.  

- **Lasso Regression (L1 penalty)**:  
  \[
  J(\theta) = \text{MSE} + \lambda \sum_j |\theta_j|
  \]  
  Shrinks coefficients + performs **feature selection** (forces some to 0).  

- **Hyperparameter Œª (alpha)**:  
  - Large ‚Üí Higher bias, less variance.  
  - Small ‚Üí Lower bias, higher variance.  

### Assumptions of Linear Models
- Linearity  
- Normality of errors  
- Feature standardization (zero mean, unit variance)  
- Handle multicollinearity (drop correlated features).  

---

## 4. Logistic Regression & Classification Metrics
‚è±Ô∏è *[56:05 ‚Üí 01:27:44]*

- **Sigmoid Function**:  
  \[
  \sigma(z) = \frac{1}{1 + e^{-z}}
  \]  
  Maps output ‚Üí (0,1).  

- **Cost Function (Log-Loss)**:  
  \[
  J(\theta) = -\frac{1}{m} \sum \big[ y \log(h_\theta(x)) + (1-y)\log(1-h_\theta(x)) \big]
  \]  
  Convex ‚Üí No local minima.  

### Confusion Matrix Metrics
- **Accuracy** = (TP + TN) / Total  
- **Precision** = TP / (TP + FP)  
- **Recall (Sensitivity)** = TP / (TP + FN)  
- **F1-Score** = 2 * (Precision * Recall) / (Precision + Recall)  

üìå Use cases:  
- Spam detection ‚Üí prioritize precision.  
- Cancer diagnosis ‚Üí prioritize recall.  

---

## 5. Practical Implementations
‚è±Ô∏è *[01:27:44 ‚Üí 02:14:09]*

- **Linear, Ridge, Lasso Regression**:  
  - Datasets: *Boston Housing*.  
  - Tools: `train_test_split`, `cross_val_score`, `GridSearchCV`.  

- **Logistic Regression**:  
  - Dataset: *Breast Cancer*.  
  - Metrics: Confusion matrix, precision, recall, F1.  

- **Naive Bayes**:  
  - Based on **Bayes‚Äô theorem**:  
    \[
    P(A|B) = \frac{P(B|A)P(A)}{P(B)}
    \]  
  - Assumes feature independence.  
  - Example: Tennis dataset (categorical probabilities).  

- **K-Nearest Neighbors (KNN)**:  
  - Predict label by **majority of k neighbors**.  
  - Distance: Euclidean, Manhattan.  
  - Sensitive to outliers & scaling.  

- **Decision Trees**:  
  - Splits data based on **Entropy or Gini impurity**.  
  - Entropy:  
    \[
    H(p) = -p \log_2 p - (1-p) \log_2 (1-p)
    \]  
  - Gini:  
    \[
    G = 1 - \sum p_i^2
    \]  
  - **Pruning** avoids overfitting.  

- **Ensemble Methods**:  
  - **Bagging** (Bootstrap Aggregation) ‚Üí Random Forest.  
  - **Boosting** ‚Üí Sequential, focus on hard examples.  

- **AdaBoost**  
  - Weak learners = decision stumps.  
  - Iteratively reweight misclassified samples.  

- **XGBoost**  
  - Optimized Gradient Boosting.  
  - Uses regularization, residuals, similarity scores.  

- **Support Vector Machines (SVM)**  
  - Separates classes with **maximum-margin hyperplane**.  
  - Controlled by penalty parameter **C**.  
  - **Kernel trick** ‚Üí Handle non-linear data.  

---

## 6. Clustering Algorithms
‚è±Ô∏è *[02:14:09 ‚Üí 02:56:00]*

### K-Means Clustering
- Iterative algorithm: Initialize centroids ‚Üí Assign points ‚Üí Recompute ‚Üí Repeat.  
- **Elbow Method**: Plot WCSS vs k ‚Üí Find "elbow".  
- **Silhouette Score**:  
  \[
  S = \frac{b - a}{\max(a, b)}
  \]  
  - Near +1 ‚Üí Good clusters.  

### Hierarchical Clustering
- Builds nested clusters using **dendrogram**.  
- Cut longest vertical line without crossing ‚Üí Optimal clusters.  
- Expensive ‚Üí Better for small datasets.  

### DBSCAN
- Density-Based Spatial Clustering.  
- Parameters:  
  - **Œµ (epsilon)** = neighborhood radius.  
  - **minPts** = minimum neighbors.  
- Finds arbitrarily shaped clusters.  
- Handles noise/outliers better than k-means.  

---

## 7. Bias‚ÄìVariance Tradeoff

- **Bias**: Error due to simplifying assumptions.  
  - High Bias = Underfitting.  

- **Variance**: Error due to sensitivity to training data.  
  - High Variance = Overfitting.  

üìå **Goal**: Find balance between bias and variance.  

---

## 8. Summary

- Covered **Supervised, Unsupervised, and Ensemble learning**.  
- Regression ‚Üí Linear, Ridge, Lasso.  
- Classification ‚Üí Logistic, Naive Bayes, KNN, Decision Trees, SVM.  
- Ensemble ‚Üí Bagging, Boosting, XGBoost.  
- Clustering ‚Üí K-Means, Hierarchical, DBSCAN.  
- Key metrics: R¬≤, Adjusted R¬≤, Precision, Recall, F1, Silhouette Score.  
- Concepts: Bias‚ÄìVariance, Overfitting, Underfitting.  

üìå These form the **core ML toolkit for interviews & real-world projects**.  

---
